{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from nltk.corpus import stopwords\n",
    "from collections import OrderedDict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the newsgroup files and storing in data_dictionary in shuffled order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dictionary={}\n",
    "data_dictionary=load_files(r\"20_newsgroups\",shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_dictionary.keys() are dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every subfolder in the given dataset corresponds to a class. Hence the target_names stores the list  of  all classes.\n",
    "data[i] is a document and target[i] is storing the subfolder/class corresponding to data[i] (to be found in the target_names list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionary['target_names'][dictionary['target'][0]]\n",
    "#dictionary['data'][dictionary['target'][0]]\n",
    "#len(dictionary['data']) =19997 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(data_dictionary, count_of_train): #takes data and length (how long  till consider data for building vocab i.e how long till training)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    vocab={} #Building Vocabulary\n",
    "    for i in range(0,count_of_train):\n",
    "        current_data=data_dictionary['data'][i] #a_random_training_document in bytes format\n",
    "        current_data=current_data.decode('utf-8','replace') #converted to unicode string format\n",
    "        #current_classname=data_dictionary['target_names'][data_dictionary['target'][i]] \n",
    "        for word in current_data.split(): #traversing  word by word (splitted on space)\n",
    "            if word in stop_words: #Ignoring stop words\n",
    "                continue\n",
    "            if word in vocab: #Existing  \n",
    "                vocab[word]=vocab[word]+1  #store in vocab\n",
    "            else:\n",
    "                vocab[word]=1 #insert in vocab\n",
    "    #Vocabulary builded\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(vocab,top_limit):\n",
    "    sortedvocab=OrderedDict(sorted(vocab.items(),key=lambda t : t[1],reverse=True)) #storing in orderedDICT in desc order of value\n",
    "    i=0\n",
    "    feature_names=[]\n",
    "    for key,value in sortedvocab.items():\n",
    "        if i==top_limit: # considering top_limit MAX OCCURING words as part of vocab\n",
    "            break\n",
    "        feature_names.append(key)\n",
    "        i=i+1\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_2D_general_input_format(data_dictionary,feature_names,start,end):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for docno in range(0,end-start-1):\n",
    "        current_data=data_dictionary['data'][start+docno]\n",
    "        current_data=current_data.decode('utf-8','replace')  \n",
    "        current_classname=data_dictionary['target_names'][data_dictionary['target'][start+docno]] #fetching class corresponding to this doc\n",
    "        X.append([])\n",
    "        Y.append(data_dictionary['target'][start+docno]) #class_number of currentclass (0-19)\n",
    "        X[docno]=[0]*len(feature_names)\n",
    "        #storing  the count of all the vocab words for all the documents\n",
    "        for word in current_data.split():\n",
    "            if word in feature_names:\n",
    "                index=feature_names.index(word)\n",
    "                X[docno][index]+=1 \n",
    "    return np.array(X),np.array(Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2000)\n",
      "(5000,)\n",
      "(14997, 2000)\n",
      "(14997,)\n"
     ]
    }
   ],
   "source": [
    "#SPLITTING 0-14997 training and 14997-19997 TESTING \n",
    "vocab=build_vocab(data_dictionary,14997)\n",
    "\n",
    "feature_names=find_features(vocab,2000) \n",
    "# top 2000 words considered as features\n",
    "\n",
    "X_test,Y_test=convert_to_2D_general_input_format(data_dictionary,feature_names,14997,19998)\n",
    "\n",
    "\n",
    "X_train,Y_train=convert_to_2D_general_input_format(data_dictionary,feature_names,0,14998)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATAPREPROCESSING DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X_train,Y_train):\n",
    "    #builds a count dictionary storing the count of particular word in particular class in training data\n",
    "    count={}\n",
    "    class_values=set(Y_train)\n",
    "    for current_class in class_values:\n",
    "        count[current_class]={}\n",
    "        count['total_data']=len(Y_train)\n",
    "        current_class_rows=(Y_train==current_class)\n",
    "        #DATA CORRESPODNDING TO CURRENT_CLASS\n",
    "        X_train_current=X_train[current_class_rows]\n",
    "        Y_train_current=Y_train[current_class_rows]\n",
    "        num_features=X_train.shape[1]\n",
    "        count[current_class]['total_words']=0\n",
    "        for wordi in range(num_features):\n",
    "            count[current_class][wordi]=X_train_current[:,wordi].sum()\n",
    "            count[current_class]['total_words']+=count[current_class][wordi]\n",
    "    return count\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability(count,x,current_class):\n",
    "        #returns probability value for the current_class for this row x (this document)\n",
    "        prob_current_class= np.log(count[current_class][\"total_words\"]) - np.log(count[\"total_data\"])\n",
    "        prob=prob_current_class\n",
    "        num_features=len(count[current_class].keys())-1\n",
    "        for wordi in range(num_features):\n",
    "            probcount_wordi_in_current_class=count[current_class][wordi]+1 #laplace_correction\n",
    "            total_words_in_current_class=count[current_class]['total_words']+num_features\n",
    "            current_prob_wordi=np.log(probcount_wordi_in_current_class)-np.log(total_words_in_current_class)\n",
    "            prob=prob+(current_prob_wordi)*x[wordi] #for every occurance of the  word finding  probability\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictSinglePoint(count,x):\n",
    "    #returns the  maximum probability class for this row x (this document)\n",
    "    classes=count.keys()\n",
    "    best_p=-1000\n",
    "    best_class=-1\n",
    "    first_run=True\n",
    "    for  current_class in classes:\n",
    "        if  (current_class=='total_data'):\n",
    "            continue\n",
    "        p_current_class=probability(count,x,current_class)\n",
    "        if (first_run or  p_current_class>best_p): #If found better class then update\n",
    "            best_p=p_current_class\n",
    "            best_class=current_class\n",
    "        first_run=False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(count,X_test):\n",
    "    #returns a list of predicted  classes for entire testing  data\n",
    "    y_pred=[]\n",
    "    for x in X_test:\n",
    "        x_class=predictSinglePoint(count,x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FITTING THE TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count=fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTING USING OWN NAIVE_BAYES PREDICT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimated time-15 minutes to predict. \n",
    "Y_pred_own=predict(count,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPORT OF SELF-MADE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.72      0.71       279\n",
      "          1       0.74      0.71      0.72       263\n",
      "          2       0.81      0.78      0.79       236\n",
      "          3       0.87      0.82      0.84       277\n",
      "          4       0.84      0.88      0.86       260\n",
      "          5       0.88      0.74      0.80       242\n",
      "          6       0.65      0.87      0.75       247\n",
      "          7       0.75      0.91      0.82       242\n",
      "          8       0.80      0.91      0.85       224\n",
      "          9       0.79      0.93      0.86       247\n",
      "         10       0.94      0.70      0.80       226\n",
      "         11       0.94      0.82      0.88       247\n",
      "         12       0.70      0.83      0.76       240\n",
      "         13       0.87      0.80      0.83       239\n",
      "         14       0.81      0.88      0.84       267\n",
      "         15       0.95      0.98      0.97       234\n",
      "         16       0.66      0.81      0.73       245\n",
      "         17       0.94      0.71      0.81       245\n",
      "         18       0.70      0.48      0.57       276\n",
      "         19       0.54      0.48      0.51       264\n",
      "\n",
      "avg / total       0.79      0.78      0.78      5000\n",
      "\n",
      "[[202   0   1   0   0   0   2   2   5   3   0   2   2   3   3   1   6   1\n",
      "    2  44]\n",
      " [  1 187   9   5   7  12  14   6   2   0   1   0  12   0   7   0   0   0\n",
      "    0   0]\n",
      " [  0  13 183   9   4   6   9   3   0   0   0   1   3   1   0   0   4   0\n",
      "    0   0]\n",
      " [  0   6   6 227  10   3  10   1   1   0   1   1   7   2   2   0   0   0\n",
      "    0   0]\n",
      " [  0   2   3   4 229   1   9   4   2   0   1   1   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1  15  15   4   8 178   6   0   2   0   0   0  10   0   3   0   0   0\n",
      "    0   0]\n",
      " [  0   4   2   4   4   0 215   4   0   1   0   1   6   1   2   1   2   0\n",
      "    0   0]\n",
      " [  0   0   1   0   1   0   5 220   3   0   0   1   6   0   1   0   0   0\n",
      "    2   2]\n",
      " [  2   1   0   0   0   0   2   9 204   2   0   0   0   1   0   0   0   0\n",
      "    3   0]\n",
      " [  1   0   0   0   0   0   4   1   2 230   6   0   1   0   0   0   1   0\n",
      "    0   1]\n",
      " [  0   0   1   0   0   0   2   3   7  44 159   0   6   0   0   0   1   0\n",
      "    2   1]\n",
      " [  2   7   0   1   0   1   3   2   4   1   0 202  10   0   5   0   3   1\n",
      "    3   2]\n",
      " [  0   1   1   5   3   1  12   7   3   0   1   0 200   2   4   0   0   0\n",
      "    0   0]\n",
      " [  8   6   1   0   1   0   6   5   4   0   0   0   6 192   4   0   1   0\n",
      "    1   4]\n",
      " [  1   9   0   1   0   0   3   3   1   1   0   0   7   4 234   0   1   0\n",
      "    0   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   2 230   0   1\n",
      "    0   0]\n",
      " [  0   0   0   1   1   0   2   5   2   2   0   2   0   2   3   0 198   1\n",
      "   11  15]\n",
      " [  8   1   0   0   3   1   9   3   3   2   0   1   4   4   8   0   5 173\n",
      "   15   5]\n",
      " [ 10   1   0   0   0   0  10   7   7   5   0   2   0   7   5   0  53   6\n",
      "  133  30]\n",
      " [ 55   1   2   0   1   0   6   7   2   0   1   0   2   2   5  10  25   1\n",
      "   18 126]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import  classification_report,confusion_matrix\n",
    "print(classification_report(Y_test,Y_pred_own))\n",
    "print(confusion_matrix(Y_test,Y_pred_own))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        MARKED DOWN THE RESULT FOR REFERENCE (SELF_MADE)\n",
    "        precision    recall  f1-score   support\n",
    "\n",
    "          0       0.69      0.72      0.71       279\n",
    "          1       0.74      0.71      0.72       263\n",
    "          2       0.81      0.78      0.79       236\n",
    "          3       0.87      0.82      0.84       277\n",
    "          4       0.84      0.88      0.86       260\n",
    "          5       0.88      0.74      0.80       242\n",
    "          6       0.65      0.87      0.75       247\n",
    "          7       0.75      0.91      0.82       242\n",
    "          8       0.80      0.91      0.85       224\n",
    "          9       0.79      0.93      0.86       247\n",
    "         10       0.94      0.70      0.80       226\n",
    "         11       0.94      0.82      0.88       247\n",
    "         12       0.70      0.83      0.76       240\n",
    "         13       0.87      0.80      0.83       239\n",
    "         14       0.81      0.88      0.84       267\n",
    "         15       0.95      0.98      0.97       234\n",
    "         16       0.66      0.81      0.73       245\n",
    "         17       0.94      0.71      0.81       245\n",
    "         18       0.70      0.48      0.57       276\n",
    "         19       0.54      0.48      0.51       264\n",
    "\n",
    "avg / total       0.79      0.78      0.78      5000\n",
    "\n",
    "[[202   0   1   0   0   0   2   2   5   3   0   2   2   3   3   1   6   1\n",
    "    2  44]\n",
    " [  1 187   9   5   7  12  14   6   2   0   1   0  12   0   7   0   0   0\n",
    "    0   0]\n",
    " [  0  13 183   9   4   6   9   3   0   0   0   1   3   1   0   0   4   0\n",
    "    0   0]\n",
    " [  0   6   6 227  10   3  10   1   1   0   1   1   7   2   2   0   0   0\n",
    "    0   0]\n",
    " [  0   2   3   4 229   1   9   4   2   0   1   1   4   0   0   0   0   0\n",
    "    0   0]\n",
    " [  1  15  15   4   8 178   6   0   2   0   0   0  10   0   3   0   0   0\n",
    "    0   0]\n",
    " [  0   4   2   4   4   0 215   4   0   1   0   1   6   1   2   1   2   0\n",
    "    0   0]\n",
    " [  0   0   1   0   1   0   5 220   3   0   0   1   6   0   1   0   0   0\n",
    "    2   2]\n",
    " [  2   1   0   0   0   0   2   9 204   2   0   0   0   1   0   0   0   0\n",
    "    3   0]\n",
    " [  1   0   0   0   0   0   4   1   2 230   6   0   1   0   0   0   1   0\n",
    "    0   1]\n",
    " [  0   0   1   0   0   0   2   3   7  44 159   0   6   0   0   0   1   0\n",
    "    2   1]\n",
    " [  2   7   0   1   0   1   3   2   4   1   0 202  10   0   5   0   3   1\n",
    "    3   2]\n",
    " [  0   1   1   5   3   1  12   7   3   0   1   0 200   2   4   0   0   0\n",
    "    0   0]\n",
    " [  8   6   1   0   1   0   6   5   4   0   0   0   6 192   4   0   1   0\n",
    "    1   4]\n",
    " [  1   9   0   1   0   0   3   3   1   1   0   0   7   4 234   0   1   0\n",
    "    0   2]\n",
    " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   2 230   0   1\n",
    "    0   0]\n",
    " [  0   0   0   1   1   0   2   5   2   2   0   2   0   2   3   0 198   1\n",
    "   11  15]\n",
    " [  8   1   0   0   3   1   9   3   3   2   0   1   4   4   8   0   5 173\n",
    "   15   5]\n",
    " [ 10   1   0   0   0   0  10   7   7   5   0   2   0   7   5   0  53   6\n",
    "  133  30]\n",
    " [ 55   1   2   0   1   0   6   7   2   0   1   0   2   2   5  10  25   1\n",
    "   18 126]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTING USING INBUILT MULTINOMIAL_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPORT OF INBUILT CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.72      0.71       279\n",
      "          1       0.75      0.71      0.73       263\n",
      "          2       0.81      0.78      0.80       236\n",
      "          3       0.87      0.82      0.84       277\n",
      "          4       0.84      0.88      0.86       260\n",
      "          5       0.88      0.71      0.79       242\n",
      "          6       0.63      0.87      0.73       247\n",
      "          7       0.75      0.90      0.82       242\n",
      "          8       0.79      0.92      0.85       224\n",
      "          9       0.79      0.93      0.85       247\n",
      "         10       0.94      0.70      0.81       226\n",
      "         11       0.95      0.81      0.88       247\n",
      "         12       0.68      0.83      0.75       240\n",
      "         13       0.87      0.80      0.83       239\n",
      "         14       0.82      0.88      0.85       267\n",
      "         15       0.95      0.98      0.97       234\n",
      "         16       0.66      0.80      0.72       245\n",
      "         17       0.94      0.70      0.80       245\n",
      "         18       0.71      0.48      0.57       276\n",
      "         19       0.55      0.49      0.52       264\n",
      "\n",
      "avg / total       0.79      0.78      0.78      5000\n",
      "\n",
      "[[201   0   2   0   0   0   2   3   6   2   0   2   3   3   3   1   6   1\n",
      "    1  43]\n",
      " [  1 187   9   5   7  11  14   6   2   0   1   0  14   0   6   0   0   0\n",
      "    0   0]\n",
      " [  0  10 184   9   4   6  11   3   0   0   0   1   3   1   0   0   4   0\n",
      "    0   0]\n",
      " [  0   6   6 226  10   3  12   1   1   0   1   1   7   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   3   4 229   1  11   3   2   0   0   1   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1  14  14   5   9 173   7   0   3   0   0   0  12   0   4   0   0   0\n",
      "    0   0]\n",
      " [  0   3   2   4   4   0 216   5   0   1   0   0   6   1   2   1   2   0\n",
      "    0   0]\n",
      " [  0   0   1   0   1   0   5 218   5   0   0   1   6   0   1   0   0   0\n",
      "    2   2]\n",
      " [  1   1   0   0   0   0   2   9 205   2   0   0   0   1   0   0   0   0\n",
      "    3   0]\n",
      " [  1   0   0   0   0   0   4   1   2 229   6   0   2   0   0   0   1   0\n",
      "    0   1]\n",
      " [  0   0   1   0   1   0   2   2   7  44 159   0   6   0   0   0   1   0\n",
      "    2   1]\n",
      " [  2   7   0   1   0   1   4   2   4   1   0 201  10   0   5   0   3   1\n",
      "    3   2]\n",
      " [  0   1   1   5   3   1  12   7   3   0   1   0 200   2   4   0   0   0\n",
      "    0   0]\n",
      " [  8   6   1   0   1   0   6   5   4   0   0   0   6 192   4   0   1   0\n",
      "    1   4]\n",
      " [  1   9   0   1   0   0   3   3   1   1   0   0   7   4 234   0   1   0\n",
      "    0   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   2 230   0   1\n",
      "    0   0]\n",
      " [  0   0   0   1   1   0   4   5   2   2   0   2   1   2   2   0 196   1\n",
      "   11  15]\n",
      " [  8   1   0   0   2   1  11   3   3   2   0   1   5   4   7   0   4 172\n",
      "   15   6]\n",
      " [ 10   1   0   0   0   0  10   8   7   5   0   2   0   7   5   0  53   6\n",
      "  132  30]\n",
      " [ 54   1   2   0   1   0   6   7   2   0   1   0   2   2   5  10  25   1\n",
      "   16 129]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_pred))\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "     MARKED DOWN THE RESULT OF INBUILD MNB FOR REFERENCE\n",
    "     precision    recall  f1-score   support\n",
    "\n",
    "          0       0.70      0.72      0.71       279\n",
    "          1       0.75      0.71      0.73       263\n",
    "          2       0.81      0.78      0.80       236\n",
    "          3       0.87      0.82      0.84       277\n",
    "          4       0.84      0.88      0.86       260\n",
    "          5       0.88      0.71      0.79       242\n",
    "          6       0.63      0.87      0.73       247\n",
    "          7       0.75      0.90      0.82       242\n",
    "          8       0.79      0.92      0.85       224\n",
    "          9       0.79      0.93      0.85       247\n",
    "         10       0.94      0.70      0.81       226\n",
    "         11       0.95      0.81      0.88       247\n",
    "         12       0.68      0.83      0.75       240\n",
    "         13       0.87      0.80      0.83       239\n",
    "         14       0.82      0.88      0.85       267\n",
    "         15       0.95      0.98      0.97       234\n",
    "         16       0.66      0.80      0.72       245\n",
    "         17       0.94      0.70      0.80       245\n",
    "         18       0.71      0.48      0.57       276\n",
    "         19       0.55      0.49      0.52       264\n",
    "\n",
    "avg / total       0.79      0.78      0.78      5000\n",
    "\n",
    "[[201   0   2   0   0   0   2   3   6   2   0   2   3   3   3   1   6   1\n",
    "    1  43]\n",
    " [  1 187   9   5   7  11  14   6   2   0   1   0  14   0   6   0   0   0\n",
    "    0   0]\n",
    " [  0  10 184   9   4   6  11   3   0   0   0   1   3   1   0   0   4   0\n",
    "    0   0]\n",
    " [  0   6   6 226  10   3  12   1   1   0   1   1   7   2   1   0   0   0\n",
    "    0   0]\n",
    " [  0   2   3   4 229   1  11   3   2   0   0   1   4   0   0   0   0   0\n",
    "    0   0]\n",
    " [  1  14  14   5   9 173   7   0   3   0   0   0  12   0   4   0   0   0\n",
    "    0   0]\n",
    " [  0   3   2   4   4   0 216   5   0   1   0   0   6   1   2   1   2   0\n",
    "    0   0]\n",
    " [  0   0   1   0   1   0   5 218   5   0   0   1   6   0   1   0   0   0\n",
    "    2   2]\n",
    " [  1   1   0   0   0   0   2   9 205   2   0   0   0   1   0   0   0   0\n",
    "    3   0]\n",
    " [  1   0   0   0   0   0   4   1   2 229   6   0   2   0   0   0   1   0\n",
    "    0   1]\n",
    " [  0   0   1   0   1   0   2   2   7  44 159   0   6   0   0   0   1   0\n",
    "    2   1]\n",
    " [  2   7   0   1   0   1   4   2   4   1   0 201  10   0   5   0   3   1\n",
    "    3   2]\n",
    " [  0   1   1   5   3   1  12   7   3   0   1   0 200   2   4   0   0   0\n",
    "    0   0]\n",
    " [  8   6   1   0   1   0   6   5   4   0   0   0   6 192   4   0   1   0\n",
    "    1   4]\n",
    " [  1   9   0   1   0   0   3   3   1   1   0   0   7   4 234   0   1   0\n",
    "    0   2]\n",
    " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   2 230   0   1\n",
    "    0   0]\n",
    " [  0   0   0   1   1   0   4   5   2   2   0   2   1   2   2   0 196   1\n",
    "   11  15]\n",
    " [  8   1   0   0   2   1  11   3   3   2   0   1   5   4   7   0   4 172\n",
    "   15   6]\n",
    " [ 10   1   0   0   0   0  10   8   7   5   0   2   0   7   5   0  53   6\n",
    "  132  30]\n",
    " [ 54   1   2   0   1   0   6   7   2   0   1   0   2   2   5  10  25   1\n",
    "   16 129]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
